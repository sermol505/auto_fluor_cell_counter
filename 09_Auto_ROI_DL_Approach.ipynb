{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ideal Processing Pipeline for Consistent CTCF in Fluorescence Microscopy\n",
    "\n",
    "Here's a comprehensive pipeline for analyzing fluorescence microscopy images with consistent CTCF measurement across different conditions and microscopes:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import necessary libraries\n",
    "import time, os, sys\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.figure import Figure\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg\n",
    "import tifffile as tiff\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage import measure, draw, exposure\n",
    "from skimage.transform import resize\n",
    "from skimage.segmentation import find_boundaries\n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import KMeans\n",
    "import scipy.stats\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import glob\n",
    "import gc\n",
    "\n",
    "mpl.rcParams['figure.dpi'] = 200\n",
    "\n",
    "from AutoImgUtils import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2025 NVIDIA Corporation\n",
      "Built on Fri_Feb_21_20:42:46_Pacific_Standard_Time_2025\n",
      "Cuda compilation tools, release 12.8, V12.8.93\n",
      "Build cuda_12.8.r12.8/compiler.35583870_0\n",
      "Wed Apr  9 20:44:44 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 572.61                 Driver Version: 572.61         CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA TITAN RTX             WDDM  |   00000000:81:00.0 Off |                  N/A |\n",
      "| 41%   38C    P8             22W /  280W |     591MiB /  24576MiB |     16%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A            9644    C+G   C:\\Windows\\explorer.exe               N/A      |\n",
      "|    0   N/A  N/A            9844    C+G   ...es\\Microsoft VS Code\\Code.exe      N/A      |\n",
      "|    0   N/A  N/A           10120    C+G   ...App_cw5n1h2txyewy\\LockApp.exe      N/A      |\n",
      "|    0   N/A  N/A           10732    C+G   ...h_cw5n1h2txyewy\\SearchApp.exe      N/A      |\n",
      "|    0   N/A  N/A           10884    C+G   ...y\\StartMenuExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A           11520    C+G   ....0.3124.93\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A           11644    C+G   ...h_cw5n1h2txyewy\\SearchApp.exe      N/A      |\n",
      "|    0   N/A  N/A           12708    C+G   ...5n1h2txyewy\\TextInputHost.exe      N/A      |\n",
      "|    0   N/A  N/A           13632    C+G   ...crosoft\\OneDrive\\OneDrive.exe      N/A      |\n",
      "|    0   N/A  N/A           13784    C+G   ...8bbwe\\PhoneExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A           15156    C+G   ...rage Manager\\JRE\\bin\\java.exe      N/A      |\n",
      "|    0   N/A  N/A           16356    C+G   ...Chrome\\Application\\chrome.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      ">>> GPU activated? YES\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version\n",
    "!nvidia-smi\n",
    "\n",
    "import os, shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cellpose import core, utils, io, models, metrics, denoise\n",
    "from glob import glob\n",
    "\n",
    "use_GPU = core.use_gpu()\n",
    "yn = ['NO', 'YES']\n",
    "print(f'>>> GPU activated? {yn[use_GPU]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization functions\n",
    "Here the visualization functions are defined, that can be used to save plots regarding the background substraction as well as the segmentation results for quality control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_background_mask(channel_image, bg_model, output_path, n_components=3, enhance_contrast=True):\n",
    "    \"\"\"Visualize background mask from GMM model with distribution plots\"\"\"\n",
    "    # Create figure with 4 subplots (2x2 grid)\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12), gridspec_kw={'height_ratios': [3, 1]})\n",
    "    \n",
    "    # Enhance contrast for visualization if requested\n",
    "    if enhance_contrast:\n",
    "        # Use percentile-based contrast stretching (robust to outliers)\n",
    "        p_low, p_high = 2, 98  # Percentiles for contrast stretching\n",
    "        display_img = exposure.rescale_intensity(\n",
    "            channel_image, \n",
    "            in_range=tuple(np.percentile(channel_image, (p_low, p_high))),\n",
    "            out_range='dtype'\n",
    "        )\n",
    "    else:\n",
    "        display_img = channel_image\n",
    "    \n",
    "    # Original image with enhanced contrast\n",
    "    axes[0, 0].imshow(display_img, cmap='gray')\n",
    "    axes[0, 0].set_title('Original Channel' + (' (Contrast Enhanced)' if enhance_contrast else ''))\n",
    "    plt.colorbar(axes[0, 0].get_images()[0], ax=axes[0, 0])\n",
    "    \n",
    "    # Background mask\n",
    "    axes[0, 1].imshow(bg_model['mask'], cmap='hot')\n",
    "    axes[0, 1].set_title(f'Background Mask\\nMean: {bg_model[\"mean\"]:.2f}, Std: {bg_model[\"std\"]:.2f}')\n",
    "    plt.colorbar(axes[0, 1].get_images()[0], ax=axes[0, 1])\n",
    "    \n",
    "    # Original with background highlighted\n",
    "    norm_img = (display_img - np.min(display_img)) / (np.max(display_img) - np.min(display_img))\n",
    "    rgb_img = np.stack([norm_img, norm_img, norm_img], axis=-1)\n",
    "    \n",
    "    # Highlight background in red\n",
    "    rgb_img[:,:,0][bg_model['mask']] = 1.0  # Set red high for background\n",
    "    rgb_img[:,:,1][bg_model['mask']] = 0.0  # Set green low for background\n",
    "    rgb_img[:,:,2][bg_model['mask']] = 0.0  # Set blue low for background\n",
    "    \n",
    "    axes[1, 0].imshow(rgb_img)\n",
    "    axes[1, 0].set_title('Background Regions (Red)')\n",
    "    \n",
    "    # Plot intensity histogram with GMM distributions\n",
    "    if 'gmm' in bg_model:\n",
    "        gmm = bg_model['gmm']\n",
    "        flat_img = channel_image.flatten()\n",
    "        \n",
    "        # Plot histogram\n",
    "        hist_range = (np.min(flat_img), np.max(flat_img))\n",
    "        n_bins = 100\n",
    "        axes[1, 1].hist(flat_img, bins=n_bins, range=hist_range, density=True, \n",
    "                       alpha=0.6, color='gray', label='Pixel Intensity')\n",
    "        \n",
    "        # Create x values for plotting GMM curves\n",
    "        x = np.linspace(hist_range[0], hist_range[1], 1000)\n",
    "        x_reshaped = x.reshape(-1, 1)\n",
    "        \n",
    "        # Plot the individual components\n",
    "        colors = ['blue', 'green', 'red', 'purple', 'orange']\n",
    "        bg_component = np.argmin(gmm.means_.flatten())\n",
    "        \n",
    "        for i in range(gmm.n_components):\n",
    "            # Calculate component density\n",
    "            weight = gmm.weights_[i]\n",
    "            mean = gmm.means_[i, 0]\n",
    "            std = np.sqrt(gmm.covariances_[i, 0, 0])\n",
    "            \n",
    "            # Create a normal distribution for this component\n",
    "            y = weight * scipy.stats.norm.pdf(x, mean, std)\n",
    "            \n",
    "            # Plot with higher alpha for background component\n",
    "            alpha = 0.8 if i == bg_component else 0.5\n",
    "            label = f\"Background (μ={mean:.1f})\" if i == bg_component else f\"Component {i+1} (μ={mean:.1f})\"\n",
    "            axes[1, 1].plot(x, y, color=colors[i % len(colors)], \n",
    "                          alpha=alpha, linewidth=2, label=label)\n",
    "        \n",
    "        axes[1, 1].set_title('Pixel Intensity Distribution')\n",
    "        axes[1, 1].set_xlabel('Pixel Value')\n",
    "        axes[1, 1].set_xscale('log')\n",
    "        axes[1, 1].set_ylabel('Density')\n",
    "        axes[1, 1].legend()\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=200)\n",
    "    plt.close('all')\n",
    "\n",
    "def create_visualization(image, masks, measurements, output_path, debug=False):\n",
    "    \"\"\"\n",
    "    Create multi-panel visualization for QC with optimized memory usage and enhanced contrast\n",
    "    \n",
    "    Parameters:\n",
    "    - image: Multi-channel image\n",
    "    - masks: Cell segmentation masks\n",
    "    - measurements: Cell measurements\n",
    "    - output_path: Where to save the visualization\n",
    "    - debug: Enable detailed timing and progress tracking\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if debug:\n",
    "            start_time = time.time()\n",
    "            print(f\"Starting visualization for {output_path}...\")\n",
    "        \n",
    "        # Force garbage collection before starting\n",
    "        gc.collect()\n",
    "        \n",
    "        # Get image dimensions and channel count\n",
    "        h, w, n_channels = image.shape\n",
    "        \n",
    "        # Calculate reasonable figure size to avoid excessive memory usage\n",
    "        max_dim = 2000  # Maximum dimension in pixels\n",
    "        scale_factor = min(1.0, max_dim / max(h, w))\n",
    "        \n",
    "        # Create figure without displaying (reduces memory usage)\n",
    "        dpi = 300  # Mantain good quality\n",
    "        fig_width = (n_channels + 1) * 4  # 4 inches per panel\n",
    "        fig_height = 4  # Fixed height\n",
    "        \n",
    "        # Use Figure directly instead of pyplot to avoid memory leaks\n",
    "        fig = Figure(figsize=(fig_width, fig_height), dpi=dpi)\n",
    "        canvas = FigureCanvasAgg(fig)\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"Created figure with size {fig_width}x{fig_height} inches at {dpi} DPI\")\n",
    "            print(f\"Processing {n_channels} channels and {len(measurements)} cells\")\n",
    "        \n",
    "        # Create subplot grid\n",
    "        grid = fig.add_gridspec(1, n_channels + 1)\n",
    "        \n",
    "        # Plot segmentation mask (first panel) with enhanced contrast\n",
    "        if debug:\n",
    "            print(\"Rendering segmentation mask...\")\n",
    "        \n",
    "        ax = fig.add_subplot(grid[0, 0])\n",
    "        \n",
    "        # Convert mask to float for better visualization\n",
    "        mask_display = (masks > 0).astype(float)\n",
    "        # Apply contrast enhancement to make it more visible\n",
    "        mask_display = exposure.equalize_adapthist(mask_display)\n",
    "        ax.imshow(mask_display, cmap='viridis')  # Use viridis for better contrast\n",
    "        ax.set_title('Cell Segmentation (Enhanced)')\n",
    "        ax.axis('off')  # Turn off axes to save memory\n",
    "        \n",
    "        # Only add labels for a subset of cells\n",
    "        if len(measurements) > 0:\n",
    "            if debug:\n",
    "                print(\"Adding cell labels...\")\n",
    "            # Select a random subset of 50 cells or fewer if there are less than 50\n",
    "            num_labels = min(50, len(measurements))\n",
    "            # Use numpy's random choice if measurements is a list, otherwise select first num_labels\n",
    "            if isinstance(measurements, list):\n",
    "                indices = np.random.choice(len(measurements), num_labels, replace=False)\n",
    "                label_subset = [measurements[i] for i in indices]\n",
    "            else:\n",
    "                label_subset = measurements[:num_labels]\n",
    "                \n",
    "            for cell in label_subset:\n",
    "                y, x = cell['centroid']\n",
    "                ax.text(x, y, str(cell['label']), color='red', fontsize=5)\n",
    "        \n",
    "        # Process channels with progress tracking\n",
    "        channel_range = range(n_channels)\n",
    "        if debug:\n",
    "            from tqdm import tqdm\n",
    "            channel_range = tqdm(channel_range, desc=\"Processing channels\")\n",
    "        \n",
    "        for ch_idx, ch in enumerate(channel_range):\n",
    "            if debug:\n",
    "                ch_start = time.time()\n",
    "                \n",
    "            # Create subplot for this channel\n",
    "            ax = fig.add_subplot(grid[0, ch_idx + 1])\n",
    "            \n",
    "            # Get channel data and apply adaptive contrast enhancement\n",
    "            channel_data = image[:,:,ch].copy()  # Make a copy to avoid modifying original\n",
    "            \n",
    "            # Adaptive histogram equalization - best for visualizing local features\n",
    "            enhanced_data = exposure.equalize_adapthist(channel_data, clip_limit=0.03)\n",
    "            \n",
    "            # Display the image\n",
    "            ax.imshow(enhanced_data, cmap='hot')\n",
    "            ax.set_title(f'Channel {ch+1} (Enhanced)')\n",
    "            ax.axis('off')  # Turn off axes to save memory\n",
    "            \n",
    "            # Show cell boundaries efficiently\n",
    "            boundaries = find_boundaries(masks > 0)\n",
    "            ax.imshow(boundaries, alpha=0.3, cmap='cool')\n",
    "            \n",
    "            # Free memory\n",
    "            del channel_data\n",
    "            del enhanced_data\n",
    "            del boundaries\n",
    "            \n",
    "            if debug:\n",
    "                print(f\"  Channel {ch+1} rendered in {time.time() - ch_start:.2f}s\")\n",
    "        \n",
    "        # Adjust layout and save\n",
    "        if debug:\n",
    "            print(\"Saving figure...\")\n",
    "            save_start = time.time()\n",
    "            \n",
    "        fig.tight_layout()\n",
    "        fig.savefig(output_path, bbox_inches='tight')\n",
    "        \n",
    "        # Clean up matplotlib resources explicitly\n",
    "        fig.clf()\n",
    "        canvas.renderer.clear()\n",
    "        del fig\n",
    "        del canvas\n",
    "        \n",
    "        # Force garbage collection\n",
    "        gc.collect()\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"Visualization saved in {time.time() - save_start:.2f}s\")\n",
    "            print(f\"Total visualization time: {time.time() - start_time:.2f}s\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in visualization: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        # Ensure cleanup even on error\n",
    "        if 'fig' in locals():\n",
    "            fig.clf()\n",
    "            del fig\n",
    "        if 'canvas' in locals():\n",
    "            canvas.renderer.clear()\n",
    "            del canvas\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background substraction and CellPose based Segmentation Functions\n",
    "Here the main functions for background substraction and cell segmentation based on de cyto3 model are defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_experiments_batch(main_directory, config):\n",
    "    \"\"\"\n",
    "    Process all experiments in the main directory with improved memory management\n",
    "    \"\"\"\n",
    "    # Create results directory\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    results_dir = os.path.join(main_directory, f\"CTCF_Analysis_{timestamp}\")\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    # Find all experiment folders\n",
    "    experiment_folders = [f.path for f in os.scandir(main_directory) if f.is_dir() \n",
    "                         and not f.name.startswith('.') and not \"CTCF_Analysis\" in f.name]\n",
    "    \n",
    "    # Process each experiment folder\n",
    "    for experiment_folder in experiment_folders:\n",
    "        exp_name = os.path.basename(experiment_folder)\n",
    "        print(f\"\\nProcessing experiment: {exp_name}\")\n",
    "        \n",
    "        # Create experiment results folder\n",
    "        exp_results_dir = os.path.join(results_dir, exp_name)\n",
    "        os.makedirs(exp_results_dir, exist_ok=True)\n",
    "        \n",
    "        # Process all images in experiment folder\n",
    "        image_files = [f for f in os.listdir(experiment_folder) \n",
    "                      if f.endswith(('.tif', '.tiff')) and not f.startswith('.')]\n",
    "        \n",
    "        # Create a DataFrame for experiment results instead of list to save memory\n",
    "        exp_results_df = pd.DataFrame()\n",
    "        \n",
    "        for image_file in image_files:\n",
    "            # Full image processing pipeline for each image\n",
    "            img_result = process_single_image(\n",
    "                os.path.join(experiment_folder, image_file),\n",
    "                exp_results_dir,\n",
    "                config\n",
    "            )\n",
    "            \n",
    "            # Append result to DataFrame directly\n",
    "            exp_results_df = pd.concat([exp_results_df, pd.DataFrame([img_result])], ignore_index=True)\n",
    "            \n",
    "            # Force garbage collection after each image\n",
    "            gc.collect()\n",
    "        \n",
    "        # Save experiment results\n",
    "        exp_results_df.to_csv(os.path.join(exp_results_dir, f\"{exp_name}_results.csv\"), index=False)\n",
    "        \n",
    "        # Write to batch summary file incrementally instead of keeping in memory\n",
    "        batch_summary_path = os.path.join(results_dir, \"batch_summary.csv\")\n",
    "        if os.path.exists(batch_summary_path):\n",
    "            exp_results_df.to_csv(batch_summary_path, mode='a', header=False, index=False)\n",
    "        else:\n",
    "            exp_results_df.to_csv(batch_summary_path, index=False)\n",
    "        \n",
    "        # Clear DataFrame to free memory\n",
    "        del exp_results_df\n",
    "        gc.collect()\n",
    "    \n",
    "    print(f\"All experiments processed. Results saved to: {results_dir}\")\n",
    "    return results_dir\n",
    "\n",
    "def resample_image(image, scale_factor=0.5):\n",
    "    \"\"\"Resample image by the given scale factor\"\"\"\n",
    "    \n",
    "    # Get original dimensions\n",
    "    h, w, c = image.shape\n",
    "    \n",
    "    # Calculate new dimensions\n",
    "    new_h, new_w = int(h * scale_factor), int(w * scale_factor)\n",
    "    \n",
    "    # Resize image\n",
    "    resized = resize(image, (new_h, new_w, c), preserve_range=True, anti_aliasing=True)\n",
    "    \n",
    "    return resized.astype(image.dtype)\n",
    "\n",
    "\n",
    "def process_single_image(image_path, output_dir, config):\n",
    "    \"\"\"Process a single fluorescence microscopy image with improved memory management\"\"\"\n",
    "    try:\n",
    "        # Load image and normalize channels\n",
    "        image = tiff.imread(image_path)\n",
    "        img_name = os.path.basename(image_path)\n",
    "        img_base = os.path.splitext(img_name)[0]\n",
    "\n",
    "        print(f\"\\nProcessing image: {img_name}\")\n",
    "\n",
    "        # Extract debug flag from config\n",
    "        debug = config.get('debug', False)\n",
    "        \n",
    "        # Move the shortest axis (channels) to the last index\n",
    "        shortest_axis = np.argmin(image.shape)\n",
    "        image = np.moveaxis(image, shortest_axis, -1)\n",
    "        \n",
    "        # Extract configuration\n",
    "        channels_of_interest = config.get('channels_of_interest', list(range(image.shape[-1])))\n",
    "        \n",
    "        # 1. BACKGROUND ESTIMATION USING GMM\n",
    "        print(\"Estimating background using GMM...\")\n",
    "        bg_models = {}\n",
    "        for ch in tqdm(range(image.shape[-1]), desc=\"Background estimation\", leave=False):\n",
    "            # Process one channel at a time to reduce memory usage\n",
    "            channel_data = image[:,:,ch].copy()  # Make a copy to avoid reference issues\n",
    "            bg_models[ch] = fast_estimate_background_gmm(channel_data)\n",
    "            \n",
    "            # Save background mask visualization if needed\n",
    "            if config.get('visualize_steps', True):\n",
    "                visualize_background_mask(channel_data, bg_models[ch], \n",
    "                                         os.path.join(output_dir, f\"{img_base}_bg_mask_ch{ch+1}.png\"))\n",
    "                plt.close('all')  # Ensure all plots are closed\n",
    "            \n",
    "            # Remove channel data from memory\n",
    "            del channel_data\n",
    "        \n",
    "        print(\"Background estimation complete.\")\n",
    "        \n",
    "        # 2. CELL SEGMENTATION USING CELLPOSE\n",
    "        cell_masks = segment_cells_with_downsampling(image, config, bg_models)\n",
    "        \n",
    "        # 3. MEASURE CTCF FOR EACH CELL AND CHANNEL\n",
    "        cell_measurements = measure_cells_ctcf(image, cell_masks, bg_models)\n",
    "        \n",
    "        # Store key results in a DataFrame and save immediately\n",
    "        cell_df = pd.DataFrame([\n",
    "            {\n",
    "                'image': img_name,\n",
    "                'cell_id': i,\n",
    "                'area': cell['area'],\n",
    "                **{f'channel_{ch+1}_ctcf': cell['ctcf'][ch] for ch in channels_of_interest},\n",
    "                **{f'channel_{ch+1}_mean': cell['mean'][ch] for ch in channels_of_interest},\n",
    "                'centroid_x': cell['centroid'][0],\n",
    "                'centroid_y': cell['centroid'][1]\n",
    "            }\n",
    "            for i, cell in enumerate(cell_measurements)\n",
    "        ])\n",
    "        cell_df.to_csv(os.path.join(output_dir, f\"{img_base}_cells.csv\"), index=False)\n",
    "        \n",
    "        # 4. GENERATE VISUALIZATIONS - do this after saving measurements to free memory\n",
    "        if debug:\n",
    "            print(f\"Starting visualization with debug mode...\")\n",
    "        \n",
    "        create_visualization(image, cell_masks, cell_measurements, \n",
    "                             os.path.join(output_dir, f\"{img_name}_analysis.png\"),\n",
    "                             debug=debug)\n",
    "        \n",
    "        # Explicitly close all plots and collect garbage\n",
    "        plt.close('all')\n",
    "        gc.collect()\n",
    "        \n",
    "        # 5. CLEANUP AND RETURN RESULTS\n",
    "        # Create a minimal results dictionary with just the summary stats\n",
    "        results = {\n",
    "            'image_name': img_name,\n",
    "            'total_cells': len(cell_measurements),\n",
    "            'image_path': image_path,\n",
    "        }\n",
    "        \n",
    "        # Add summarized measurements\n",
    "        for ch in channels_of_interest:\n",
    "            ch_ctcf = [cell['ctcf'][ch] for cell in cell_measurements]\n",
    "            results[f'channel_{ch+1}_mean_ctcf'] = np.mean(ch_ctcf)\n",
    "            results[f'channel_{ch+1}_median_ctcf'] = np.median(ch_ctcf)\n",
    "            results[f'channel_{ch+1}_std_ctcf'] = np.std(ch_ctcf)\n",
    "            \n",
    "        # Clean up large objects\n",
    "        del image, cell_masks, cell_measurements, bg_models, cell_df\n",
    "        gc.collect()\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        gc.collect()\n",
    "        return {'image_name': os.path.basename(image_path), 'error': str(e), 'total_cells': 0}\n",
    "\n",
    "def fast_estimate_background_gmm_gpu(channel_image, n_components=3, sample_ratio=0.1):\n",
    "    \"\"\"GPU-accelerated background estimation using GMM\"\"\"\n",
    "    \n",
    "    # Convert to torch tensor and flatten\n",
    "    device = torch.device('cuda')\n",
    "    flat_img_gpu = torch.from_numpy(channel_image.flatten()).float().to(device)\n",
    "    \n",
    "    # Random sampling on GPU for efficiency\n",
    "    n_samples = max(10000, int(sample_ratio * flat_img_gpu.size(0)))\n",
    "    indices = torch.randperm(flat_img_gpu.size(0), device=device)[:n_samples]\n",
    "    sample_data_gpu = flat_img_gpu[indices].reshape(-1, 1).cpu().numpy()\n",
    "    \n",
    "    # The GMM calculation itself stays on CPU as sklearn doesn't support GPU\n",
    "    # But data preparation and sampling are accelerated\n",
    "    gmm = GaussianMixture(n_components=n_components, random_state=42, max_iter=100)\n",
    "    gmm.fit(sample_data_gpu)\n",
    "    \n",
    "    # Process results on GPU where possible\n",
    "    means = torch.tensor(gmm.means_.flatten(), device=device)\n",
    "    bg_component = torch.argmin(means).item()\n",
    "    bg_mean = means[bg_component].item()\n",
    "    bg_std = float(np.sqrt(gmm.covariances_[bg_component].flatten()[0]))\n",
    "    \n",
    "    # Predict on GPU for mask creation\n",
    "    # This is a hybrid approach: get scores from CPU model but process on GPU\n",
    "    pixel_labels_cpu = gmm.predict(flat_img_gpu.cpu().reshape(-1, 1).numpy())\n",
    "    pixel_labels_gpu = torch.from_numpy(pixel_labels_cpu).to(device)\n",
    "    bg_mask = (pixel_labels_gpu == bg_component).reshape(channel_image.shape)\n",
    "    \n",
    "    # Return result including CPU-based GMM for compatibility\n",
    "    return {\n",
    "        'mean': bg_mean,\n",
    "        'std': bg_std,\n",
    "        'mask': bg_mask.cpu().numpy(),\n",
    "        'gmm': gmm,\n",
    "        'bg_percentage': (torch.sum(bg_mask).item() / bg_mask.numel() * 100),\n",
    "        'component_means': gmm.means_.flatten(),\n",
    "        'n_components': n_components,\n",
    "        'bg_component': bg_component\n",
    "    }\n",
    "\n",
    "def fast_estimate_background_gmm(channel_image, n_components=3, sample_ratio=0.1, max_iter=100):\n",
    "    \"\"\"\n",
    "    Faster background estimation using GMM with downsampling\n",
    "    \n",
    "    Parameters:\n",
    "    - channel_image: 2D array with image channel\n",
    "    - n_components: Number of GMM components\n",
    "    - sample_ratio: Fraction of pixels to sample (smaller = faster)\n",
    "    - max_iter: Maximum EM iterations\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Flatten image\n",
    "        flat_img = channel_image.flatten()\n",
    "        \n",
    "        # Downsample by random sampling (much faster and uses less memory)\n",
    "        n_samples = max(10000, int(sample_ratio * flat_img.size))\n",
    "        indices = np.random.choice(flat_img.size, size=n_samples, replace=False)\n",
    "        sample_data = flat_img[indices].reshape(-1, 1)\n",
    "        \n",
    "        # Initialize with K-means for faster convergence\n",
    "        kmeans = KMeans(n_clusters=n_components, n_init=1, max_iter=100)\n",
    "        kmeans.fit(sample_data)\n",
    "        \n",
    "        # Free up memory from kmeans\n",
    "        kmeans_centers = kmeans.cluster_centers_.copy()\n",
    "        del kmeans\n",
    "        \n",
    "        # Configure GMM with performance parameters\n",
    "        gmm = GaussianMixture(\n",
    "            n_components=n_components,\n",
    "            random_state=42,\n",
    "            n_init=1,\n",
    "            max_iter=max_iter,\n",
    "            tol=1e-3,\n",
    "            means_init=kmeans_centers\n",
    "        )\n",
    "        \n",
    "        # Fit on sample data\n",
    "        gmm.fit(sample_data)\n",
    "        \n",
    "        # Extract and store only what we need from GMM\n",
    "        means = gmm.means_.flatten().copy()\n",
    "        bg_component = np.argmin(means)\n",
    "        bg_mean = means[bg_component]\n",
    "        bg_std = np.sqrt(gmm.covariances_[bg_component].flatten()[0])\n",
    "        \n",
    "        # Predict on full image for mask (this is memory intensive)\n",
    "        pixel_labels = gmm.predict(flat_img.reshape(-1, 1))\n",
    "        bg_mask = (pixel_labels == bg_component).reshape(channel_image.shape)\n",
    "        \n",
    "        # Free memory\n",
    "        del sample_data, flat_img, pixel_labels\n",
    "        \n",
    "        # Store only what we need, not the full GMM model\n",
    "        result = {\n",
    "            'mean': bg_mean,\n",
    "            'std': bg_std,\n",
    "            'mask': bg_mask,\n",
    "            'gmm': gmm,  # Store the fitted model for later use if needed\n",
    "            'bg_percentage': np.sum(bg_mask) / bg_mask.size * 100,\n",
    "            'component_means': means,\n",
    "            'n_components': n_components,\n",
    "            'bg_component': bg_component\n",
    "        }\n",
    "        \n",
    "        # No need to store the full GMM model\n",
    "        del gmm\n",
    "        gc.collect()\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in GMM background estimation: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        # Return fallback values\n",
    "        return {'mean': 0, 'std': 0, 'mask': np.zeros_like(channel_image, dtype=bool)}\n",
    "\n",
    "def estimate_background_gmm(channel_image, n_components=3, adaptive=False, max_components=6):\n",
    "    \"\"\"\n",
    "    Estimate background using Gaussian Mixture Model\n",
    "    \n",
    "    Parameters:\n",
    "    - channel_image: 2D array with image channel\n",
    "    - n_components: Number of GMM components (default=3)\n",
    "    - adaptive: If True, select optimal components using BIC (default=False)\n",
    "    - max_components: Maximum number of components to try if adaptive=True\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary with background statistics and fitted GMM model\n",
    "    \"\"\"\n",
    "    \n",
    "    # Flatten image for GMM\n",
    "    flat_img = channel_image.flatten().reshape(-1, 1)\n",
    "    \n",
    "    # If adaptive component selection is requested\n",
    "    if adaptive:\n",
    "        bic_scores = []\n",
    "        models = []\n",
    "        \n",
    "        # Try different numbers of components\n",
    "        for n in range(1, max_components + 1):\n",
    "            gmm = GaussianMixture(n_components=n, random_state=42, n_init=3)\n",
    "            gmm.fit(flat_img)\n",
    "            bic_scores.append(gmm.bic(flat_img))\n",
    "            models.append(gmm)\n",
    "        \n",
    "        # Select model with lowest BIC score\n",
    "        best_idx = np.argmin(bic_scores)\n",
    "        gmm = models[best_idx]\n",
    "        n_components = best_idx + 1  # Update n_components to the selected value\n",
    "        print(f\"Adaptive GMM selected {n_components} components with BIC: {bic_scores[best_idx]:.2f}\")\n",
    "    else:\n",
    "        # Fit GMM model with specified components\n",
    "        gmm = GaussianMixture(n_components=n_components, random_state=42, n_init=3)\n",
    "        gmm.fit(flat_img)\n",
    "    \n",
    "    # Identify background component (lowest mean)\n",
    "    means = gmm.means_.flatten()\n",
    "    bg_component = np.argmin(means)\n",
    "    \n",
    "    # Get background statistics\n",
    "    bg_mean = means[bg_component]\n",
    "    bg_std = np.sqrt(gmm.covariances_[bg_component].flatten()[0])\n",
    "    \n",
    "    # Create background mask \n",
    "    pixel_labels = gmm.predict(flat_img)\n",
    "    bg_mask = (pixel_labels == bg_component).reshape(channel_image.shape)\n",
    "    \n",
    "    # Calculate background percentage\n",
    "    bg_percentage = np.sum(bg_mask) / bg_mask.size * 100\n",
    "    \n",
    "    return {\n",
    "        'mean': bg_mean,\n",
    "        'std': bg_std,\n",
    "        'mask': bg_mask,\n",
    "        'gmm': gmm,  # Store the fitted model\n",
    "        'bg_percentage': bg_percentage,\n",
    "        'component_means': means,\n",
    "        'n_components': n_components,\n",
    "        'bg_component': bg_component\n",
    "    }\n",
    "\n",
    "def segment_cells_cellpose(image, config, bg_models=None):\n",
    "\n",
    "    \"\"\"Cell segmentation using CellPose with optimized memory usage\"\"\"\n",
    "    io.logger_setup()\n",
    "    \n",
    "    # Extract the right channels for segmentation\n",
    "    cyto_channel_idx = config.get('cytoplasm_channel', 4) - 1 \n",
    "    nuc_channel_idx = config.get('nucleus_channel', 1) - 1\n",
    "    \n",
    "    # Choose model based on segmentation type\n",
    "    if config.get('segmentation_type') == 'nuclei_only':\n",
    "        model = models.Cellpose(gpu=config.get('use_gpu', True), model_type='nuclei')\n",
    "        cellpose_channels = [0, 0]  # Standard for nuclei model\n",
    "        \n",
    "        # For single-channel segmentation\n",
    "        img_to_segment = image[:,:,nuc_channel_idx].copy()\n",
    "        \n",
    "        # Apply background subtraction if needed\n",
    "        if bg_models is not None and nuc_channel_idx in bg_models:\n",
    "            ch_mean = bg_models[nuc_channel_idx]['mean']\n",
    "            img_to_segment = np.clip(img_to_segment - ch_mean, 0, None)\n",
    "            print(f\"Channel {nuc_channel_idx}: Subtracted mean background {ch_mean:.2f}\")\n",
    "            \n",
    "    else:\n",
    "        model = models.Cellpose(gpu=config.get('use_gpu', True), model_type=\"cyto3\")\n",
    "        cellpose_channels = [2, 1]  # Standard for cyto model: 1=nuclei, 2=cyto\n",
    "        \n",
    "        # Create channels array with proper mapping between original and stacked indices\n",
    "        channel_mapping = [cyto_channel_idx, nuc_channel_idx] \n",
    "        \n",
    "        # Create the segmentation image \n",
    "        img_to_segment = np.stack([image[:,:,nuc_channel_idx], image[:,:,cyto_channel_idx]], axis=-1).copy()\n",
    "        \n",
    "        # Apply background subtraction with correct index mapping\n",
    "        if bg_models is not None:\n",
    "            for i, orig_idx in enumerate(channel_mapping):\n",
    "                if orig_idx in bg_models:\n",
    "                    ch_mean = bg_models[orig_idx]['mean']\n",
    "                    img_to_segment[:,:,i] = np.clip(img_to_segment[:,:,i] - ch_mean, 0, None)\n",
    "                    print(f\"Channel {orig_idx} (stack position {i}): Subtracted mean background {ch_mean:.2f}\")\n",
    "    \n",
    "    print(f\"Running Cellpose with channels: {cellpose_channels}\")\n",
    "    print(f\"Image shape for segmentation: {img_to_segment.shape}\")\n",
    "    \n",
    "    # Run segmentation with debug info\n",
    "    try:\n",
    "        masks, flows, styles, diams = model.eval(\n",
    "            img_to_segment, \n",
    "            channels=cellpose_channels,\n",
    "            diameter=config.get('cell_diameter', 20.0),\n",
    "            flow_threshold=config.get('flow_threshold', 0.4),\n",
    "            cellprob_threshold=config.get('cellprob_threshold', 0.0),\n",
    "            normalize=True,\n",
    "            progress=True\n",
    "        )\n",
    "        \n",
    "        # Free memory\n",
    "        del img_to_segment\n",
    "        if 'flows' in locals() and flows is not None:\n",
    "            del flows\n",
    "        if 'styles' in locals() and styles is not None:\n",
    "            del styles\n",
    "        if 'diams' in locals() and diams is not None:\n",
    "            del diams\n",
    "        gc.collect()\n",
    "        \n",
    "        print(f\"Segmentation complete! Found {len(np.unique(masks))-1} objects\")\n",
    "        return masks\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR in Cellpose: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        return np.zeros(image.shape[:2], dtype=np.int32)\n",
    "\n",
    "def segment_cells_with_downsampling(image, config, bg_models=None):\n",
    "    \"\"\"Segment cells with better memory management\"\"\"\n",
    "    # Get downsampling factor from config\n",
    "    downsample_factor = config.get('downsample_factor', 1.0)\n",
    "    \n",
    "    try:\n",
    "        if downsample_factor >= 1.0:\n",
    "            # Process at original resolution\n",
    "            masks = segment_cells_cellpose(image, config, bg_models)\n",
    "            return masks\n",
    "        \n",
    "        # Downsample image for processing\n",
    "        small_image = resample_image(image, downsample_factor)\n",
    "        \n",
    "        # Adjust cell diameter for downsampled image\n",
    "        small_config = config.copy()\n",
    "        small_config['cell_diameter'] = config.get('cell_diameter', 20.0) * downsample_factor\n",
    "        \n",
    "        # Run segmentation on smaller image\n",
    "        small_masks = segment_cells_cellpose(small_image, small_config, bg_models)\n",
    "        \n",
    "        # Free memory before upsampling\n",
    "        del small_image\n",
    "        gc.collect()\n",
    "        \n",
    "        # Upsample masks to original size\n",
    "        masks_upscaled = resize(small_masks, image.shape[0:2], order=0, preserve_range=True)\n",
    "        masks_upscaled = masks_upscaled.astype(np.int32)\n",
    "        \n",
    "        # Free memory\n",
    "        del small_masks\n",
    "        \n",
    "        return masks_upscaled\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in segmentation: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        # Return empty mask in case of error\n",
    "        return np.zeros(image.shape[:2], dtype=np.int32)\n",
    "\n",
    "def measure_cells_ctcf(image, cell_masks, bg_models, debug=False):\n",
    "    \"\"\"\n",
    "    Measure CTCF for all cells and channels with progress tracking and debugging\n",
    "    \n",
    "    Parameters:\n",
    "    - image: Multi-channel image array\n",
    "    - cell_masks: Integer mask with cell labels\n",
    "    - bg_models: Background models for each channel\n",
    "    - debug: Enable detailed timing and debugging outputs\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Get properties of each cell\n",
    "    props = measure.regionprops(cell_masks)\n",
    "    measurements = []\n",
    "    \n",
    "    # Create progress bar\n",
    "    total_cells = len(props)\n",
    "    print(f\"Measuring CTCF for {total_cells} cells across {image.shape[-1]} channels...\")\n",
    "    \n",
    "    # Process cells with progress bar\n",
    "    for i, prop in enumerate(tqdm(props, desc=\"Measuring cells\", leave=False)):\n",
    "        cell_start = time.time()\n",
    "        \n",
    "        # Basic cell properties\n",
    "        cell_data = {\n",
    "            'label': prop.label,\n",
    "            'area': prop.area,\n",
    "            'centroid': prop.centroid,\n",
    "            'ctcf': {},\n",
    "            'mean': {},\n",
    "            'total': {},\n",
    "            'bg_value': {}\n",
    "        }\n",
    "        \n",
    "        # Get cell mask (this can be optimized by using prop.coords directly)\n",
    "        mask = cell_masks == prop.label\n",
    "        \n",
    "        # Process all channels with a nested progress bar if debugging\n",
    "        channels_iter = tqdm(range(image.shape[-1]), desc=f\"Cell {i+1}/{total_cells} channels\", \n",
    "                            leave=False, disable=not debug)\n",
    "        \n",
    "        for ch in channels_iter:\n",
    "            # Extract channel and cell region\n",
    "            channel = image[:,:,ch]\n",
    "            \n",
    "            # Use vectorized operation instead of creating intermediate mask\n",
    "            # This is faster than doing channel[mask]\n",
    "            cell_region = np.take(channel, prop.coords[:, 0], axis=0)\n",
    "            cell_region = np.take(cell_region, prop.coords[:, 1], axis=1)\n",
    "            \n",
    "            # Background value\n",
    "            bg_value = bg_models[ch]['mean']\n",
    "            cell_data['bg_value'][ch] = bg_value\n",
    "            \n",
    "            # Calculate measurements\n",
    "            total_intensity = np.sum(cell_region)\n",
    "            mean_intensity = np.mean(cell_region)\n",
    "            \n",
    "            # CTCF = Integrated Density - (Area × Mean Background)\n",
    "            ctcf = total_intensity - (prop.area * bg_value)\n",
    "            \n",
    "            # Store results\n",
    "            cell_data['total'][ch] = total_intensity\n",
    "            cell_data['mean'][ch] = mean_intensity\n",
    "            cell_data['ctcf'][ch] = ctcf\n",
    "            \n",
    "        measurements.append(cell_data)\n",
    "        \n",
    "        # Print debug info periodically\n",
    "        if debug and (i == 0 or i == total_cells-1 or i % max(1, total_cells//100) == 0):\n",
    "            cell_time = time.time() - cell_start\n",
    "            print(f\"  Cell {i+1}/{total_cells} processed in {cell_time:.3f}s\")\n",
    "    \n",
    "    # Final timing\n",
    "    total_time = time.time() - start_time\n",
    "    cells_per_sec = total_cells / total_time\n",
    "    print(f\"CTCF measurement complete: {total_cells} cells in {total_time:.2f}s ({cells_per_sec:.2f} cells/sec)\")\n",
    "    \n",
    "    return measurements\n",
    "\n",
    "def measure_cells_ctcf_gpu(image, cell_masks, bg_models, debug=False):\n",
    "    \"\"\"GPU-accelerated CTCF measurement for improved performance\"\"\"\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Convert arrays to PyTorch tensors on GPU\n",
    "    device = torch.device('cuda')\n",
    "    image_tensor = torch.from_numpy(image).to(device)\n",
    "    mask_tensor = torch.from_numpy(cell_masks).to(device)\n",
    "    \n",
    "    # Get unique cell IDs for processing\n",
    "    cell_ids = torch.unique(mask_tensor)[1:]  # Skip 0 (background)\n",
    "    total_cells = len(cell_ids)\n",
    "    print(f\"Measuring CTCF for {total_cells} cells across {image.shape[-1]} channels on GPU...\")\n",
    "    \n",
    "    # Prepare results container\n",
    "    measurements = []\n",
    "    \n",
    "    # Process in batches if there are many cells\n",
    "    batch_size = 500  # Adjust based on your GPU memory\n",
    "    for batch_start in tqdm(range(0, total_cells, batch_size), desc=\"Processing cell batches\"):\n",
    "        batch_end = min(batch_start + batch_size, total_cells)\n",
    "        batch_ids = cell_ids[batch_start:batch_end]\n",
    "        \n",
    "        # Process each cell in the batch (can parallelize this loop too)\n",
    "        for cell_idx in range(len(batch_ids)):\n",
    "            cell_id = batch_ids[cell_idx].item()\n",
    "            \n",
    "            # Create binary mask for this cell\n",
    "            cell_mask = (mask_tensor == cell_id)\n",
    "            cell_area = torch.sum(cell_mask).item()\n",
    "            \n",
    "            # Calculate centroid (still using CPU as this is not the bottleneck)\n",
    "            # You could implement a GPU version if needed\n",
    "            y_indices, x_indices = torch.where(cell_mask)\n",
    "            centroid_y = torch.mean(y_indices.float()).item()\n",
    "            centroid_x = torch.mean(x_indices.float()).item()\n",
    "            \n",
    "            cell_data = {\n",
    "                'label': cell_id,\n",
    "                'area': cell_area,\n",
    "                'centroid': (centroid_y, centroid_x),\n",
    "                'ctcf': {},\n",
    "                'mean': {},\n",
    "                'total': {},\n",
    "                'bg_value': {}\n",
    "            }\n",
    "            \n",
    "            # Process all channels using GPU operations\n",
    "            for ch in range(image_tensor.shape[2]):\n",
    "                channel_data = image_tensor[:, :, ch]\n",
    "                bg_value = bg_models[ch]['mean']\n",
    "                cell_data['bg_value'][ch] = bg_value\n",
    "                \n",
    "                # GPU-accelerated measurements\n",
    "                cell_pixels = channel_data[cell_mask]\n",
    "                total_intensity = torch.sum(cell_pixels).item()\n",
    "                mean_intensity = torch.mean(cell_pixels).item()\n",
    "                ctcf = total_intensity - (cell_area * bg_value)\n",
    "                \n",
    "                cell_data['total'][ch] = total_intensity\n",
    "                cell_data['mean'][ch] = mean_intensity\n",
    "                cell_data['ctcf'][ch] = ctcf\n",
    "            \n",
    "            measurements.append(cell_data)\n",
    "        \n",
    "        # Free GPU memory after each batch\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Final timing\n",
    "    total_time = time.time() - start_time\n",
    "    cells_per_sec = total_cells / total_time\n",
    "    print(f\"GPU CTCF measurement complete: {total_cells} cells in {total_time:.2f}s ({cells_per_sec:.2f} cells/sec)\")\n",
    "    \n",
    "    return measurements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crop Section testing \n",
    "In this section the functions for testing on a small cropped picture of the image are dealigned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_central_region(image, crop_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Crop the central region of an image for quick segmentation testing\n",
    "    \n",
    "    Parameters:\n",
    "    - image: The input image (H, W, C)\n",
    "    - crop_ratio: Size of the crop relative to original image (0.1 = 10%)\n",
    "    \n",
    "    Returns:\n",
    "    - cropped_image: The central cropped region\n",
    "    - crop_coords: (y_start, y_end, x_start, x_end) for reference\n",
    "    \"\"\"\n",
    "    # Get image dimensions\n",
    "    h, w, c = image.shape\n",
    "    \n",
    "    # Calculate crop size\n",
    "    crop_h = int(h * crop_ratio)\n",
    "    crop_w = int(w * crop_ratio)\n",
    "    \n",
    "    # Calculate central coordinates\n",
    "    center_y, center_x = h // 2, w // 2\n",
    "    \n",
    "    # Calculate crop boundaries\n",
    "    y_start = center_y - (crop_h // 2)\n",
    "    y_end = center_y + (crop_h // 2)\n",
    "    x_start = center_x - (crop_w // 2)\n",
    "    x_end = center_x + (crop_w // 2)\n",
    "    \n",
    "    # Ensure coordinates are within image bounds\n",
    "    y_start = max(0, y_start)\n",
    "    y_end = min(h, y_end)\n",
    "    x_start = max(0, x_start)\n",
    "    x_end = min(w, x_end)\n",
    "    \n",
    "    # Extract crop\n",
    "    cropped_image = image[y_start:y_end, x_start:x_end, :]\n",
    "    \n",
    "    return cropped_image, (y_start, y_end, x_start, x_end)    \n",
    "\n",
    "def test_segmentation_on_crop(image_path, output_dir, config, crop_ratio=0.1):\n",
    "        \"\"\"\n",
    "        Test segmentation on a central crop of an image\n",
    "        \n",
    "        Parameters:\n",
    "        - image_path: Path to the input image\n",
    "        - output_dir: Directory to save results\n",
    "        - config: Configuration dictionary\n",
    "        - crop_ratio: Size of the crop relative to original image (0.1 = 10%)\n",
    "        \n",
    "        Returns:\n",
    "        - Dictionary with segmentation results and parameters\n",
    "        \"\"\"\n",
    "        # Load image and normalize channels\n",
    "        image = tiff.imread(image_path)\n",
    "        img_name = os.path.basename(image_path)\n",
    "        img_base = os.path.splitext(img_name)[0]\n",
    "        \n",
    "        print(f\"\\nTesting segmentation on cropped region of: {img_name}\")\n",
    "        \n",
    "        # Move the shortest axis (channels) to the last index if needed\n",
    "        shortest_axis = np.argmin(image.shape)\n",
    "        image = np.moveaxis(image, shortest_axis, -1)\n",
    "        \n",
    "        # Crop central region\n",
    "        cropped_image, crop_coords = crop_central_region(image, crop_ratio)\n",
    "        y_start, y_end, x_start, x_end = crop_coords\n",
    "        \n",
    "        print(f\"Original image shape: {image.shape}\")\n",
    "        print(f\"Cropped region shape: {cropped_image.shape}\")\n",
    "        print(f\"Crop coordinates: (y={y_start}:{y_end}, x={x_start}:{x_end})\")\n",
    "        \n",
    "        # Create output directory for this test if it doesn't exist\n",
    "        crop_output_dir = os.path.join(output_dir, f\"{img_base}_crop_test\")\n",
    "        os.makedirs(crop_output_dir, exist_ok=True)\n",
    "        \n",
    "        # 1. Estimate background for the cropped region\n",
    "        print(\"Estimating background using GMM...\")\n",
    "        bg_models = {}\n",
    "        for ch in range(cropped_image.shape[-1]):\n",
    "            channel_data = cropped_image[:,:,ch].copy()\n",
    "            bg_models[ch] = fast_estimate_background_gmm(channel_data)\n",
    "            \n",
    "            # Save background mask visualization\n",
    "            visualize_background_mask(channel_data, bg_models[ch], \n",
    "                                     os.path.join(crop_output_dir, f\"crop_bg_mask_ch{ch+1}.png\"))\n",
    "        \n",
    "        # 2. Segment cells on the cropped region\n",
    "        cell_masks = segment_cells_with_downsampling(cropped_image, config, bg_models)\n",
    "        \n",
    "        # 3. Measure CTCF for each cell in the cropped region\n",
    "        cell_measurements = measure_cells_ctcf(cropped_image, cell_masks, bg_models)\n",
    "        \n",
    "        # Save visualization of the segmentation results\n",
    "        create_visualization(cropped_image, cell_masks, cell_measurements, \n",
    "                            os.path.join(crop_output_dir, f\"{img_base}_crop_segmentation.png\"), \n",
    "                            debug=True)\n",
    "        \n",
    "        # Create a comparison visualization showing where the crop is from\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Show original with crop region highlighted\n",
    "        plt.subplot(1, 2, 1)\n",
    "        # Use first channel for display or create a composite\n",
    "        if image.shape[-1] >= 3:\n",
    "            display_img = np.zeros((image.shape[0], image.shape[1], 3))\n",
    "            for i in range(min(3, image.shape[-1])):\n",
    "                ch_data = exposure.equalize_adapthist(image[:,:,i])\n",
    "                display_img[:,:,i] = ch_data\n",
    "        else:\n",
    "            display_img = exposure.equalize_adapthist(image[:,:,0])\n",
    "        \n",
    "        plt.imshow(display_img)\n",
    "        plt.gca().add_patch(plt.Rectangle((x_start, y_start), \n",
    "                                         x_end - x_start, \n",
    "                                         y_end - y_start, \n",
    "                                         fill=False, \n",
    "                                         edgecolor='red', \n",
    "                                         linewidth=2))\n",
    "        plt.title('Original Image with Crop Region')\n",
    "        \n",
    "        # Show the cropped region with segmentation overlay\n",
    "        plt.subplot(1, 2, 2)\n",
    "        # Create overlay of segmentation on image\n",
    "        if cropped_image.shape[-1] >= 3:\n",
    "            crop_display = np.zeros((cropped_image.shape[0], cropped_image.shape[1], 3))\n",
    "            for i in range(min(3, cropped_image.shape[-1])):\n",
    "                ch_data = exposure.equalize_adapthist(cropped_image[:,:,i])\n",
    "                crop_display[:,:,i] = ch_data\n",
    "        else:\n",
    "            crop_display = exposure.equalize_adapthist(cropped_image[:,:,0])\n",
    "        \n",
    "        plt.imshow(crop_display)\n",
    "        # Add cell mask overlay\n",
    "        plt.imshow(cell_masks > 0, alpha=0.7, cmap='cool')\n",
    "        plt.title(f'Segmentation on Cropped Region ({len(cell_measurements)} cells)')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(crop_output_dir, f\"{img_base}_crop_location.png\"), dpi=150)\n",
    "        plt.close()\n",
    "        \n",
    "        # Save cell measurements to CSV\n",
    "        cell_df = pd.DataFrame([\n",
    "            {\n",
    "                'cell_id': cell['label'],\n",
    "                'area': cell['area'],\n",
    "                **{f'channel_{ch+1}_ctcf': cell['ctcf'][ch] for ch in range(cropped_image.shape[-1])},\n",
    "                **{f'channel_{ch+1}_mean': cell['mean'][ch] for ch in range(cropped_image.shape[-1])},\n",
    "                'centroid_x': cell['centroid'][1],\n",
    "                'centroid_y': cell['centroid'][0]\n",
    "            }\n",
    "            for cell in cell_measurements\n",
    "        ])\n",
    "        cell_df.to_csv(os.path.join(crop_output_dir, f\"{img_base}_crop_cells.csv\"), index=False)\n",
    "        \n",
    "        # Return info about the test\n",
    "        return {\n",
    "            'image_name': img_name,\n",
    "            'crop_region': crop_coords,\n",
    "            'cell_count': len(cell_measurements),\n",
    "            'output_dir': crop_output_dir\n",
    "        }\n",
    "\n",
    "# Test segmentation on a cropped region before full processing\n",
    "def test_segmentation_parameters(image_path, config, crop_ratio=0.1):\n",
    "    \"\"\"Test segmentation parameters on a cropped region of an image\"\"\"\n",
    "    # Create a temporary output directory\n",
    "    output_dir = os.path.join(os.path.dirname(image_path), \"segmentation_tests\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Run the crop test\n",
    "    test_result = test_segmentation_on_crop(\n",
    "        image_path=image_path,\n",
    "        output_dir=output_dir,\n",
    "        config=config,\n",
    "        crop_ratio=crop_ratio\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n✓ Segmentation test complete!\")\n",
    "    print(f\"  - Found {test_result['cell_count']} cells in the cropped region\")\n",
    "    print(f\"  - Results saved to: {test_result['output_dir']}\")\n",
    "    print(\"\\nTIP: Review the results and adjust segmentation parameters in config as needed\")\n",
    "    \n",
    "    return test_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Images\n",
    "In this section firstly the configuratin will be set for the segmentation, and a test can be done for an individual file. On the second part, a batch processing of multiple files can be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the configuration for the pipeline\n",
    "config = {\n",
    "    # CellPose settings\n",
    "    'segmentation_type': 'cyto_and_nuclei',  # or 'nuclei_only'\n",
    "    'cytoplasm_channel': 4,  # Far Red channel for cytoplasm/membrane\n",
    "    'nucleus_channel': 1,    # Blue channel (DAPI) for nuclei\n",
    "    'cell_diameter': 30.0,     # Approximate diameter in pixels\n",
    "    'use_gpu': True,         # Use GPU acceleration\n",
    "    'flow_threshold': 0.4,   # Flow threshold for CellPose\n",
    "    'cellprob_threshold': 0.6,  # Cell probability threshold for CellPose\n",
    "    'downsample_factor': 0.5,  # Downsample factor for speed (1.0 = no downsampling)\n",
    "    \n",
    "    # Visualization settings\n",
    "    'visualize_steps': True,  # Set to False if you don't want intermediate visualizations\n",
    "    \n",
    "    # Other pipeline settings\n",
    "    'channels_of_interest': [0, 1, 2, 3]  # All channels to measure\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cropped segmentation and background substraction testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Selected folder: H:/01_Colaboration/02_Slide_Scanner/20250605_TestFolder/Exp1\n"
     ]
    }
   ],
   "source": [
    "folder_path_test = select_folder()\n",
    "print(f'>>> Selected folder: {folder_path_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing segmentation on cropped region of: EXP10_2_1_1_1_2_3_scFl_DoxCont_Light-OME_s5.ome.tiff\n",
      "Original image shape: (11232, 13066, 4)\n",
      "Cropped region shape: (1122, 1306, 4)\n",
      "Crop coordinates: (y=5055:6177, x=5880:7186)\n",
      "Estimating background using GMM...\n",
      "creating new log file\n",
      "2025-04-09 21:14:25,498 [INFO] WRITING LOG OUTPUT TO C:\\Users\\s.molina\\.cellpose\\run.log\n",
      "2025-04-09 21:14:25,499 [INFO] \n",
      "cellpose version: \t3.1.1.1 \n",
      "platform:       \twin32 \n",
      "python version: \t3.12.3 \n",
      "torch version:  \t2.8.0.dev20250405+cu128\n",
      "2025-04-09 21:14:25,502 [INFO] ** TORCH CUDA version installed and working. **\n",
      "2025-04-09 21:14:25,503 [INFO] >>>> using GPU (CUDA)\n",
      "2025-04-09 21:14:25,504 [INFO] >> cyto3 << model set to be used\n",
      "2025-04-09 21:14:25,601 [INFO] >>>> loading model C:\\Users\\s.molina\\.cellpose\\models\\cyto3\n",
      "2025-04-09 21:14:25,764 [INFO] >>>> model diam_mean =  30.000 (ROIs rescaled to this size during training)\n",
      "Channel 3 (stack position 0): Subtracted mean background 383.36\n",
      "Channel 0 (stack position 1): Subtracted mean background 559.53\n",
      "Running Cellpose with channels: [2, 1]\n",
      "Image shape for segmentation: (561, 653, 2)\n",
      "2025-04-09 21:14:25,772 [INFO] channels set to [2, 1]\n",
      "2025-04-09 21:14:25,773 [INFO] ~~~ FINDING MASKS ~~~\n",
      "2025-04-09 21:14:26,719 [INFO] >>>> TOTAL TIME 0.95 sec\n",
      "Segmentation complete! Found 49 objects\n",
      "Measuring CTCF for 49 cells across 4 channels...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTCF measurement complete: 49 cells in 2.44s (20.07 cells/sec)\n",
      "Starting visualization for H:/01_Colaboration/02_Slide_Scanner/20250605_TestFolder/Exp1\\segmentation_tests\\EXP10_2_1_1_1_2_3_scFl_DoxCont_Light-OME_s5.ome_crop_test\\EXP10_2_1_1_1_2_3_scFl_DoxCont_Light-OME_s5.ome_crop_segmentation.png...\n",
      "Created figure with size 20x4 inches at 300 DPI\n",
      "Processing 4 channels and 49 cells\n",
      "Rendering segmentation mask...\n",
      "Adding cell labels...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing channels:  25%|██▌       | 1/4 [00:00<00:00,  3.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Channel 1 rendered in 0.28s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing channels:  50%|█████     | 2/4 [00:00<00:00,  3.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Channel 2 rendered in 0.27s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing channels:  75%|███████▌  | 3/4 [00:00<00:00,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Channel 3 rendered in 0.26s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing channels: 100%|██████████| 4/4 [00:01<00:00,  3.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Channel 4 rendered in 0.26s\n",
      "Saving figure...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization saved in 4.91s\n",
      "Total visualization time: 6.47s\n",
      "\n",
      "✓ Segmentation test complete!\n",
      "  - Found 49 cells in the cropped region\n",
      "  - Results saved to: H:/01_Colaboration/02_Slide_Scanner/20250605_TestFolder/Exp1\\segmentation_tests\\EXP10_2_1_1_1_2_3_scFl_DoxCont_Light-OME_s5.ome_crop_test\n",
      "\n",
      "TIP: Review the results and adjust segmentation parameters in config as needed\n"
     ]
    }
   ],
   "source": [
    "# Example usage in your script\n",
    "image_paths = glob.glob(os.path.join(folder_path_test, \"*.tiff\"))\n",
    "if len(image_paths) > 0:\n",
    "    # Test segmentation on first image before batch processing\n",
    "    test_result = test_segmentation_parameters(image_paths[0], config, crop_ratio=0.1)\n",
    "    \n",
    "else:\n",
    "    print(f\"No TIFF images found in {folder_path_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_folder = select_folder()\n",
    "print(f'>>> Selected folder: {batch_folder}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all experiments in the selected folder\n",
    "batch_results = process_experiments_batch(batch_folder, config)\n",
    "print(f'>>> Batch processing complete. Results saved to: {batch_results}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".auto_img",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
